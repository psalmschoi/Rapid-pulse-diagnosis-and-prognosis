{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f777b66-b5ce-4b51-a260-0f95e8c9dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import hstack, vstack\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c268bd1-67cb-4cfb-842e-0bd9f4e75302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Ansh\\\\Desktop\\\\MEST\\\\HMG3\\\\Rapid_fin\\\\deep learning modeling'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_directory(foldername, filename = None, back_num = 0):\n",
    "    cur = os.getcwd()\n",
    "    for i in range(back_num):\n",
    "        cur = os.path.abspath(os.path.join(cur, os.pardir))\n",
    "    for folder in foldername:\n",
    "        cur = os.path.join(cur, folder)\n",
    "    if not os.path.exists(cur):\n",
    "        os.makedirs(cur)\n",
    "        print(f'{cur} created')\n",
    "    if filename != None:\n",
    "        cur = os.path.join(cur, filename)\n",
    "    return cur\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e19cef-7f59-4f9c-a102-bce664fb00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_add = find_directory(foldername = [], filename = 'SOC_Point_Data.csv')\n",
    "dat = pd.read_csv(csv_add, index_col = (0,1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51190277-29e5-4a99-bb89-b99b751c7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data(dat) :\n",
    "    \n",
    "    RPT_MODE = \"0.1C\"\n",
    "    SOC_Range = [9,10,11,12]\n",
    "\n",
    "    Time_Range = range(6, 15, 2)\n",
    "    SOC_Range = [str(i) for i in SOC_Range]\n",
    "    \n",
    "\n",
    "    Data = dat\n",
    "\n",
    "    X = Data.loc[RPT_MODE, \"0\" : \"16\"]\n",
    "    Y = Data.loc[RPT_MODE, [\"SOH\", \"Next_SOH\", \"Ratio_SOH\", \"Ratio_CYC\"]].groupby(level = [\"Next\", \"Path\", \"Number\"]).mean()\n",
    "    \n",
    "    y = pd.Series(Y[\"Next_SOH\"] - Y[\"SOH\"], name = \"Delta_SOH\")\n",
    "    \n",
    "    Y = pd.concat([Y, y], axis = 1)\n",
    "\n",
    "    X_seek = X.loc[X.index.get_level_values(\"Time\").isin(Time_Range), SOC_Range]\n",
    "\n",
    "\n",
    "    X_std = X_seek.groupby(level = [\"Next\", \"Path\", \"Number\"]).std()\n",
    "    \n",
    "    return X_std, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cdfd1a-dfb7-4066-ab61-9ef975b8d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Even_Split(X, y, test_size, rs) :\n",
    "\n",
    "    X_M = X.xs(key = 'M', level = 'Next', drop_level = False)\n",
    "    X_D = X.xs(key = 'D', level = 'Next', drop_level = False)\n",
    "    X_H = X.xs(key = 'H', level = 'Next', drop_level = False)\n",
    "    \n",
    "    XX = {\"M\" : X_M, \"D\" : X_D, \"H\" : X_H}\n",
    "    \n",
    "    y_M = y.xs(key = 'M', level = 'Next', drop_level = False)\n",
    "    y_D = y.xs(key = 'D', level = 'Next', drop_level = False)\n",
    "    y_H = y.xs(key = 'H', level = 'Next', drop_level = False)\n",
    "    \n",
    "    yy = {\"M\" : y_M, \"D\" : y_D, \"H\" : y_H}\n",
    "    \n",
    "    \n",
    "    XXX = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "    \n",
    "    yyy = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "    \n",
    "    \n",
    "    for n in [\"M\", \"D\", \"H\"] :\n",
    "        for path in range(1,5) :\n",
    "            X_path = XX[n].loc[XX[n].index.get_level_values(level = 'Path').str.len() == path]\n",
    "            y_path = yy[n].loc[yy[n].index.get_level_values(level = 'Path').str.len() == path]\n",
    "            \n",
    "            XXX[n].append(X_path)\n",
    "            yyy[n].append(y_path)\n",
    "            \n",
    "            \n",
    "    XX_tn = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "    XX_te = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "    \n",
    "    yy_tn = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "    yy_te = {\"M\" : [], \"D\" : [], \"H\" : []}\n",
    "        \n",
    "    for n in [\"M\", \"D\", \"H\"] :\n",
    "        for path in range(1,5) :\n",
    "            X_temp = XXX[n][path-1]\n",
    "            y_temp = yyy[n][path-1]\n",
    "            \n",
    "            X_tn, X_te, y_tn, y_te = train_test_split(X_temp, y_temp, test_size = test_size, random_state = rs)\n",
    "            \n",
    "            XX_tn[n].append(X_tn)\n",
    "            XX_te[n].append(X_te)\n",
    "            yy_tn[n].append(y_tn)\n",
    "            yy_te[n].append(y_te)\n",
    "                  \n",
    "    for n in [\"M\", \"D\", \"H\"] :\n",
    "        XX_tn[n] = pd.concat(XX_tn[n])\n",
    "        XX_te[n] = pd.concat(XX_te[n])\n",
    "        yy_tn[n] = pd.concat(yy_tn[n])\n",
    "        yy_te[n] = pd.concat(yy_te[n])\n",
    "        \n",
    "        \n",
    "    X_tn = pd.concat(XX_tn.values())\n",
    "    X_te = pd.concat(XX_te.values())\n",
    "    \n",
    "    y_tn = pd.concat(yy_tn.values())\n",
    "    y_te = pd.concat(yy_te.values())\n",
    "    \n",
    "    return X_tn, X_te, y_tn, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f240b499-f460-4d4d-a641-1427605d59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_tensor(index_list):\n",
    "    next_mapping = {'M': 0, 'D': 1, 'H': 2}\n",
    "    next_index = [next_mapping[idx[0]] for idx in index_list] \n",
    "    next_tensor = torch.tensor(next_index)\n",
    "    one_hot = torch.nn.functional.one_hot(next_tensor, num_classes=3).float()\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f614def-b4a4-481d-835c-7798c2ac0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRandomSeed(random_seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a293a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred, eps=1e-7):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    mask = np.abs(y_true) > eps\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def mape_by_next(y_true, y_pred, next_labels):\n",
    "    out = {}\n",
    "    for k in ['M', 'D', 'H']:\n",
    "        m = (next_labels == k)\n",
    "        out[k] = mape(y_true[m], y_pred[m])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6ff2bf-41cc-4b6a-9773-02ee2dbbf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = [100, 120, 140, 160, 180]\n",
    "\n",
    "Max_depth     = [2, 4, 6, 8, 10]\n",
    "N_estimators  = [1000]\n",
    "Subsamples    = [0.8, 0.9, 1.0]\n",
    "Colsample     = [0.9, 1.0]\n",
    "Learning_rate = [0.001, 0.01, 0.1]\n",
    "\n",
    "results_df = pd.DataFrame(columns = ['rs', 'depth', 'ne', 'sub', 'col', 'lr', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Next M MAPE', 'Next D MAPE', 'Next H MAPE'])\n",
    "\n",
    "X, y = Get_Data(dat)\n",
    "\n",
    "for rs, depth, ne, sub, col, lr in product(random_states, Max_depth, N_estimators, Subsamples, Colsample, Learning_rate):\n",
    "    setRandomSeed(rs)\n",
    "\n",
    "    X_tn, X_te, y_tn, y_te = Even_Split(X, y, 1/3, rs)\n",
    "    \n",
    "    X_tr, X_va, y_tr, y_va = Even_Split(X_tn, y_tn, 1/6, rs)\n",
    "\n",
    "    next_map = {'M': 0, 'D': 1, 'H': 2}\n",
    "    def get_next(idx):\n",
    "        labels = np.array([next_map[i] for i in idx.get_level_values(\"Next\")])\n",
    "        onehot = np.eye(3, dtype=float)[labels]\n",
    "        return onehot\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    X_tr_std   = std_scaler.fit_transform(X_tr)\n",
    "    X_val_std  = std_scaler.transform(X_va)\n",
    "    X_te_std   = std_scaler.transform(X_te)\n",
    "\n",
    "    next_train = get_next(X_tr.index)\n",
    "    next_val   = get_next(X_va.index)\n",
    "    next_test  = get_next(X_te.index)\n",
    "    \n",
    "    X_train    = np.concatenate([X_tr_std, next_train], axis = 1)\n",
    "    X_val      = np.concatenate([X_val_std, next_val], axis = 1)\n",
    "    X_test     = np.concatenate([X_te_std, next_test], axis = 1)\n",
    "    \n",
    "    y_train    = y_tr[\"Next_SOH\"].values\n",
    "    y_val      = y_va[\"Next_SOH\"].values\n",
    "    y_test     = y_te[\"Next_SOH\"].values\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth = depth,\n",
    "        n_estimators = ne,\n",
    "        learning_rate = lr,\n",
    "        subsample = sub,\n",
    "        colsample_bytree = col,\n",
    "        random_state = rs,\n",
    "        eval_metric = 'rmse',\n",
    "        early_stopping_rounds = 50\n",
    "    )\n",
    "\n",
    "    _ = model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_val, y_val)],\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    y_pred_tr = model.predict(X_train)\n",
    "    y_pred_va = model.predict(X_val)\n",
    "    y_pred_te = model.predict(X_test)\n",
    "\n",
    "    train_mape = mape(y_train, y_pred_tr)\n",
    "    val_mape   = mape(y_val,   y_pred_va)\n",
    "    test_mape  = mape(y_test,  y_pred_te)\n",
    "\n",
    "\n",
    "    next_test_labels = X_te.index.get_level_values(\"Next\").to_numpy()\n",
    "    by_next = mape_by_next(y_test, y_pred_te, next_test_labels)\n",
    "\n",
    "    row = [rs, depth, ne, sub, col, lr, train_mape, val_mape, test_mape, by_next['M'], by_next['D'], by_next['H']]\n",
    "    \n",
    "    temp_df = pd.DataFrame([row], columns=results_df.columns)\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4819d6c3-d539-4de0-9e83-ed6af617fa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>ne</th>\n",
       "      <th>sub</th>\n",
       "      <th>col</th>\n",
       "      <th>lr</th>\n",
       "      <th>Train MAPE</th>\n",
       "      <th>Val MAPE</th>\n",
       "      <th>Test MAPE</th>\n",
       "      <th>Next M MAPE</th>\n",
       "      <th>Next D MAPE</th>\n",
       "      <th>Next H MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.016107</td>\n",
       "      <td>1.092332</td>\n",
       "      <td>1.060781</td>\n",
       "      <td>1.183655</td>\n",
       "      <td>0.984109</td>\n",
       "      <td>1.014577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.687852</td>\n",
       "      <td>0.808056</td>\n",
       "      <td>0.820977</td>\n",
       "      <td>0.935189</td>\n",
       "      <td>0.756396</td>\n",
       "      <td>0.771345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.640689</td>\n",
       "      <td>0.810043</td>\n",
       "      <td>0.827024</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.781551</td>\n",
       "      <td>0.759210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.013764</td>\n",
       "      <td>1.091534</td>\n",
       "      <td>1.060064</td>\n",
       "      <td>1.181907</td>\n",
       "      <td>0.983255</td>\n",
       "      <td>1.015031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.669771</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>0.817729</td>\n",
       "      <td>0.936964</td>\n",
       "      <td>0.751382</td>\n",
       "      <td>0.764841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0.890970</td>\n",
       "      <td>0.877410</td>\n",
       "      <td>0.983996</td>\n",
       "      <td>0.834794</td>\n",
       "      <td>0.813440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.185443</td>\n",
       "      <td>0.904276</td>\n",
       "      <td>0.894729</td>\n",
       "      <td>0.992270</td>\n",
       "      <td>0.863404</td>\n",
       "      <td>0.828514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.794794</td>\n",
       "      <td>1.150288</td>\n",
       "      <td>1.074844</td>\n",
       "      <td>1.190681</td>\n",
       "      <td>1.002371</td>\n",
       "      <td>1.031480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.220841</td>\n",
       "      <td>0.959940</td>\n",
       "      <td>0.926989</td>\n",
       "      <td>1.039692</td>\n",
       "      <td>0.869580</td>\n",
       "      <td>0.871694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.187802</td>\n",
       "      <td>0.949494</td>\n",
       "      <td>0.906483</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.869396</td>\n",
       "      <td>0.850111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth    ne  sub  col     lr  Train MAPE  Val MAPE  Test MAPE  \\\n",
       "0       2  1000  0.8  0.9  0.001    1.016107  1.092332   1.060781   \n",
       "1       2  1000  0.8  0.9  0.010    0.687852  0.808056   0.820977   \n",
       "2       2  1000  0.8  0.9  0.100    0.640689  0.810043   0.827024   \n",
       "3       2  1000  0.8  1.0  0.001    1.013764  1.091534   1.060064   \n",
       "4       2  1000  0.8  1.0  0.010    0.669771  0.806887   0.817729   \n",
       "..    ...   ...  ...  ...    ...         ...       ...        ...   \n",
       "85     10  1000  1.0  0.9  0.010    0.186024  0.890970   0.877410   \n",
       "86     10  1000  1.0  0.9  0.100    0.185443  0.904276   0.894729   \n",
       "87     10  1000  1.0  1.0  0.001    0.794794  1.150288   1.074844   \n",
       "88     10  1000  1.0  1.0  0.010    0.220841  0.959940   0.926989   \n",
       "89     10  1000  1.0  1.0  0.100    0.187802  0.949494   0.906483   \n",
       "\n",
       "    Next M MAPE  Next D MAPE  Next H MAPE  \n",
       "0      1.183655     0.984109     1.014577  \n",
       "1      0.935189     0.756396     0.771345  \n",
       "2      0.940312     0.781551     0.759210  \n",
       "3      1.181907     0.983255     1.015031  \n",
       "4      0.936964     0.751382     0.764841  \n",
       "..          ...          ...          ...  \n",
       "85     0.983996     0.834794     0.813440  \n",
       "86     0.992270     0.863404     0.828514  \n",
       "87     1.190681     1.002371     1.031480  \n",
       "88     1.039692     0.869580     0.871694  \n",
       "89     0.999943     0.869396     0.850111  \n",
       "\n",
       "[90 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = results_df.groupby(['depth', 'ne', 'sub', 'col', 'lr'])[['Train MAPE', 'Val MAPE', 'Test MAPE', 'Next M MAPE', 'Next D MAPE', 'Next H MAPE']].mean().reset_index()\n",
    "summary_df\n",
    "summary_df.to_csv('XGB_next_info_sum.csv')\n",
    "results_df.to_csv('XGB_next_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
