{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb02ec6a-459b-4b1d-8195-d9c4bddd94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from numpy import hstack, vstack\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb3ccdb-189d-4c0e-a1d5-05c1d1ff4168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Ansh\\\\Desktop\\\\MEST\\\\HMG3\\\\Rapid_fin\\\\deep learning modeling'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_directory(foldername, filename = None, back_num = 0):\n",
    "    cur = os.getcwd()\n",
    "    for i in range(back_num):\n",
    "        cur = os.path.abspath(os.path.join(cur, os.pardir))\n",
    "    for folder in foldername:\n",
    "        cur = os.path.join(cur, folder)\n",
    "    if not os.path.exists(cur):\n",
    "        os.makedirs(cur)\n",
    "        print(f'{cur} created')\n",
    "    if filename != None:\n",
    "        cur = os.path.join(cur, filename)\n",
    "    return cur\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236388e4-9f8b-47a1-b29f-6f5e2e6964b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_add = find_directory(foldername = [], filename = 'SOC_Point_Data.csv')\n",
    "dat = pd.read_csv(csv_add, index_col = (0,1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d12a9ba-cfb5-4146-baf0-d0f4a94c6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_all(dat) :\n",
    "    \n",
    "    RPT_MODE = \"0.1C\"\n",
    "    SOC_Range = [9,10,11,12]\n",
    "\n",
    "    Time_Range = range(4, 1201, 2)\n",
    "    SOC_Range = [str(i) for i in SOC_Range]\n",
    "    \n",
    "    Data = dat\n",
    "\n",
    "    X = Data.loc[RPT_MODE, SOC_Range]\n",
    "    Y = Data.loc[RPT_MODE, [\"SOH\", \"Next_SOH\", \"Ratio_SOH\", \"Ratio_CYC\"]].groupby(level = [\"Next\", \"Path\", \"Number\"]).mean()\n",
    "    y = Y[\"Next_SOH\"]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8d86f6-daee-4a45-b57e-ed27379f9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Even_Split(X, y, test_size, rs):\n",
    "    XX = {n: X.xs(key=n, level='Next', drop_level=False) for n in [\"M\", \"D\", \"H\"]}\n",
    "    yy = {n: y.xs(key=n, level='Next', drop_level=False) for n in [\"M\", \"D\", \"H\"]}\n",
    "\n",
    "    XXX, yyy = {n: [] for n in [\"M\", \"D\", \"H\"]}, {n: [] for n in [\"M\", \"D\", \"H\"]}\n",
    "    for n in [\"M\", \"D\", \"H\"]:\n",
    "        for path in range(1, 5):\n",
    "            X_path = XX[n].loc[XX[n].index.get_level_values('Path').str.len() == path]\n",
    "            y_path = yy[n].loc[yy[n].index.get_level_values('Path').str.len() == path]\n",
    "\n",
    "            if 'Time' in y_path.index.names:\n",
    "                y_path = y_path.reset_index('Time', drop=True)\n",
    "\n",
    "            XXX[n].append(X_path)\n",
    "            yyy[n].append(y_path)\n",
    "\n",
    "    XX_tn, XX_te = {n: [] for n in [\"M\", \"D\", \"H\"]}, {n: [] for n in [\"M\", \"D\", \"H\"]}\n",
    "    yy_tn, yy_te = {n: [] for n in [\"M\", \"D\", \"H\"]}, {n: [] for n in [\"M\", \"D\", \"H\"]}\n",
    "\n",
    "    for n in [\"M\", \"D\", \"H\"]:\n",
    "        for path in range(1, 5):\n",
    "            X_temp = XXX[n][path-1]\n",
    "            y_temp = yyy[n][path-1]\n",
    "\n",
    "            cell_index = y_temp.index.drop_duplicates()\n",
    "\n",
    "            cells_tn, cells_te = train_test_split(cell_index, test_size=test_size, random_state=rs)\n",
    "\n",
    "            X_tn = X_temp[X_temp.index.droplevel('Time').isin(cells_tn)]\n",
    "            X_te = X_temp[X_temp.index.droplevel('Time').isin(cells_te)]\n",
    "\n",
    "            y_tn = y_temp.loc[cells_tn]\n",
    "            y_te = y_temp.loc[cells_te]\n",
    "\n",
    "            XX_tn[n].append(X_tn)\n",
    "            XX_te[n].append(X_te)\n",
    "            yy_tn[n].append(y_tn)\n",
    "            yy_te[n].append(y_te)\n",
    "\n",
    "    X_tn = pd.concat([pd.concat(XX_tn[n]) for n in [\"M\", \"D\", \"H\"]])\n",
    "    X_te = pd.concat([pd.concat(XX_te[n]) for n in [\"M\", \"D\", \"H\"]])\n",
    "    y_tn = pd.concat([pd.concat(yy_tn[n]) for n in [\"M\", \"D\", \"H\"]])\n",
    "    y_te = pd.concat([pd.concat(yy_te[n]) for n in [\"M\", \"D\", \"H\"]])\n",
    "\n",
    "    return X_tn, X_te, y_tn, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1073e7ea-87e2-4c6b-b16e-7385ba1eb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(X_tn, X_te, y_tn, y_te, batch_size=32, val_size=1/6, rs=100):\n",
    "    \"\"\"\n",
    "    X_tn, X_te: MultiIndex DataFrame (Next, Path, Number, Time)\n",
    "    y_tn, y_te: Series or DataFrame with index (Next, Path, Number)\n",
    "    \"\"\"\n",
    "\n",
    "    X_tr, X_vl, y_tr, y_vl = Even_Split(X_tn, y_tn, val_size, rs)\n",
    "    keys_train = y_tr.index\n",
    "    keys_val = y_vl.index\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tn_scaled = X_tn.copy()\n",
    "    X_te_scaled = X_te.copy()\n",
    "\n",
    "    scaler.fit(X_tn.values)\n",
    "    X_tn_scaled.loc[:, :] = scaler.transform(X_tn.values)\n",
    "    X_te_scaled.loc[:, :] = scaler.transform(X_te.values)\n",
    "\n",
    "    next_map = {'M': 0, 'D': 1, 'H': 2}\n",
    "    get_next = lambda idx: torch.nn.functional.one_hot(torch.tensor([next_map[i] for i in idx.get_level_values(\"Next\")]), num_classes=3).float()\n",
    "\n",
    "    def to_tensor(X_df, y_df, keys):\n",
    "        X_group = X_df.groupby(level=[\"Next\", \"Path\", \"Number\"])\n",
    "        X_tensor = torch.stack([\n",
    "            torch.tensor(X_group.get_group(k).values, dtype=torch.float32)\n",
    "            for k in keys\n",
    "        ])\n",
    "        y_tensor = torch.tensor(y_df.loc[keys].values, dtype=torch.float32).unsqueeze(1)\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    next_train = get_next(pd.MultiIndex.from_tuples(keys_train, names=[\"Next\", \"Path\", \"Number\"]))\n",
    "    next_val   = get_next(pd.MultiIndex.from_tuples(keys_val,   names=[\"Next\", \"Path\", \"Number\"]))\n",
    "    next_test  = get_next(y_te.index)\n",
    "    \n",
    "    X_train_tensor, y_train_tensor = to_tensor(X_tn_scaled, y_tn, keys_train)\n",
    "    X_val_tensor,   y_val_tensor   = to_tensor(X_tn_scaled, y_tn, keys_val)\n",
    "    X_test_tensor,  y_test_tensor  = to_tensor(X_te_scaled, y_te, y_te.index.drop_duplicates())\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, next_train, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val_tensor, next_val,  y_val_tensor),   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(TensorDataset(X_test_tensor, next_test,  y_test_tensor),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675b5c37-1535-41b4-a7da-9481f5b2eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRandomSeed(random_seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c24c57b-dbdd-4ee4-ac2f-1c9601c28dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, lr=1e-3, weight_decay=0, epoch=1000, patience=50, random_seed = 0):\n",
    "        self.random_seed = random_seed\n",
    "        setRandomSeed(self.random_seed)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.cri2 = MAPELoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.8)\n",
    "        self.epoch = epoch\n",
    "        self.patience = patience\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def train(self, train_loader, test_loader=None, val_loader=None):\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for ep in range(1, self.epoch + 1):\n",
    "            self.model.train()\n",
    "            for x_batch, onehot, y_batch in train_loader:\n",
    "                x_batch, onehot, y_batch = x_batch.to(self.device), onehot.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x_batch, onehot)\n",
    "                loss = self.cri2(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            avg_train_loss = self.evaluate(train_loader)\n",
    "            test_loss = self.evaluate(test_loader) if test_loader else None\n",
    "            val_loss = self.evaluate(val_loader) if val_loader else None\n",
    "\n",
    "            self.train_loss.append(avg_train_loss)\n",
    "            self.test_loss.append(test_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "\n",
    "            if val_loader:\n",
    "                if val_loss < best_val_loss - 1e-4:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict()\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= self.patience:\n",
    "                        print(f\"Early stopping at epoch {ep}\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, onehot, y_batch in data_loader:\n",
    "                x_batch, onehot, y_batch = x_batch.to(self.device), onehot.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(x_batch, onehot)\n",
    "                loss = self.cri2(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def predict(self, x, onehot):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = x.to(self.device)\n",
    "            onehot = onehot.to(self.device)\n",
    "            return self.model(x, onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63aefad-1df2-43f8-a4c7-95b1b66352eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, info, train_loader, val_loader, test_loader, plot = True):\n",
    "    rs, l1 = info\n",
    "    \n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "    \n",
    "    y_true_val = []\n",
    "    y_pred_val = []\n",
    "    \n",
    "    y_true_test = []\n",
    "    y_pred_test = []\n",
    "\n",
    "    y_true_test_M = []\n",
    "    y_pred_test_M = []\n",
    "\n",
    "    y_true_test_D = []\n",
    "    y_pred_test_D = []\n",
    "\n",
    "    y_true_test_H = []\n",
    "    y_pred_test_H = []\n",
    "    \n",
    "    for x_batch, onehot, y_batch in train_loader:\n",
    "        preds = trainer.predict(x_batch, onehot)\n",
    "        y_true_train.append(y_batch)\n",
    "        y_pred_train.append(preds.cpu())\n",
    "    \n",
    "    for x_batch, onehot, y_batch in val_loader:\n",
    "        preds = trainer.predict(x_batch, onehot)\n",
    "        y_true_val.append(y_batch)\n",
    "        y_pred_val.append(preds.cpu())\n",
    "    \n",
    "    for x_batch, onehot, y_batch in test_loader:\n",
    "        preds = trainer.predict(x_batch, onehot)\n",
    "        y_true_test.append(y_batch)\n",
    "        y_pred_test.append(preds.cpu())\n",
    "    \n",
    "        onehot_np = onehot.cpu().numpy()\n",
    "        y_true_np = y_batch.cpu().numpy()\n",
    "        y_pred_np = preds.cpu().numpy()\n",
    "    \n",
    "        for i in range(len(onehot_np)):\n",
    "            if np.array_equal(onehot_np[i], [1, 0, 0]):  # 'M'\n",
    "                y_true_test_M.append(y_true_np[i])\n",
    "                y_pred_test_M.append(y_pred_np[i])\n",
    "            elif np.array_equal(onehot_np[i], [0, 1, 0]):  # 'D'\n",
    "                y_true_test_D.append(y_true_np[i])\n",
    "                y_pred_test_D.append(y_pred_np[i])\n",
    "            elif np.array_equal(onehot_np[i], [0, 0, 1]):  # 'H'\n",
    "                y_true_test_H.append(y_true_np[i])\n",
    "                y_pred_test_H.append(y_pred_np[i])\n",
    "    \n",
    "    y_true_train = torch.cat(y_true_train).numpy()\n",
    "    y_pred_train = torch.cat(y_pred_train).numpy()\n",
    "    \n",
    "    y_true_val = torch.cat(y_true_val).numpy()\n",
    "    y_pred_val = torch.cat(y_pred_val).numpy()\n",
    "    \n",
    "    y_true_test = torch.cat(y_true_test).numpy()\n",
    "    y_pred_test = torch.cat(y_pred_test).numpy()\n",
    "\n",
    "    y_true_test_M = np.array(y_true_test_M)\n",
    "    y_pred_test_M = np.array(y_pred_test_M)\n",
    "    \n",
    "    y_true_test_D = np.array(y_true_test_D)\n",
    "    y_pred_test_D = np.array(y_pred_test_D)\n",
    "    \n",
    "    y_true_test_H = np.array(y_true_test_H)\n",
    "    y_pred_test_H = np.array(y_pred_test_H)\n",
    "    \n",
    "    mape_M = mean_absolute_percentage_error(y_true_test_M, y_pred_test_M) * 100 if len(y_true_test_M) > 0 else np.nan\n",
    "    mape_D = mean_absolute_percentage_error(y_true_test_D, y_pred_test_D) * 100 if len(y_true_test_D) > 0 else np.nan\n",
    "    mape_H = mean_absolute_percentage_error(y_true_test_H, y_pred_test_H) * 100 if len(y_true_test_H) > 0 else np.nan\n",
    "    \n",
    "    train_mape = mean_absolute_percentage_error(y_true_train, y_pred_train) * 100\n",
    "    val_mape = mean_absolute_percentage_error(y_true_val, y_pred_val) * 100\n",
    "    test_mape = mean_absolute_percentage_error(y_true_test, y_pred_test) * 100\n",
    "    print(f\"Model: {model}, rs: {rs}, l1: {l1}\\n Train MAPE: {train_mape:.2f}%, Val MAPE: {val_mape:.2f}%, Test MAPE: {test_mape:.2f}%\")\n",
    "    print(f\"Test MAPE by 'Next': M: {mape_M:.2f}%, D: {mape_D:.2f}%, H: {mape_H:.2f}%\")\n",
    "    \n",
    "    if plot == True:\n",
    "        _ = plt.figure()\n",
    "        _ = plt.scatter(y_true_train, y_pred_train, label = 'Train')\n",
    "        _ = plt.scatter(y_true_val, y_pred_val, label = 'Val')\n",
    "        _ = plt.scatter(y_true_test, y_pred_test, label = 'Test')\n",
    "        min_val = min(y_true_train.min(), y_true_test.min())\n",
    "        max_val = max(y_true_train.max(), y_true_test.max())\n",
    "        _ = plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Ideal line')\n",
    "        _ = plt.xlabel('True SOH')\n",
    "        _ = plt.ylabel('Predicted SOH')\n",
    "        _ = plt.legend()\n",
    "        _ = plt.title(f'Random state: {rs}, hid: {hid}, {nl} layer, lr: {lr}')\n",
    "    return train_mape, val_mape, test_mape, mape_M, mape_D, mape_H\n",
    "\n",
    "def plot_loss(info, train_loss, val_loss, test_loss):\n",
    "    rs, l1 = info\n",
    "    _ = plt.figure()\n",
    "    _ = plt.plot(train_loss, label = 'Train loss')\n",
    "    _ = plt.plot(val_loss, label = 'Val loss')\n",
    "    _ = plt.plot(test_loss, label = 'Test loss')\n",
    "    _ = plt.ylim([0, 2])\n",
    "    _ = plt.xlabel('Epoch')\n",
    "    _ = plt.ylabel('Loss')\n",
    "    _ = plt.legend()\n",
    "    _ = plt.title(f'Random state: {rs}, hid: {hid}, {nl} layers, lr: {lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02587287-cc8c-4e51-b971-e7d9dc777ddd",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f300ff-1eff-422c-87d6-845e07677f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, l1_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def elasticnet_penalty(self):\n",
    "        l1 = torch.norm(self.linear.weight, p=1)\n",
    "        l2 = torch.norm(self.linear.weight, p=2) ** 2\n",
    "        return self.l1_ratio * l1 + (1 - self.l1_ratio) * l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03b138d6-9cd1-4cbe-9560-8ad1815237ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=64, num_layers=2, next_dim=3):\n",
    "        super().__init__()\n",
    "        total_input = input_dim + next_dim\n",
    "        layers = [\n",
    "            nn.Linear(total_input, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]\n",
    "        layers += [nn.Linear(hidden_dim, 1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_feat, onehot):\n",
    "        x = torch.cat([x_feat, onehot], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f453ec-8366-493c-837f-4e1e22ffa8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EN_MLP(nn.Module):\n",
    "    def __init__(self, out_dim=4, hidden_dim=64, num_layers=2, next_dim=3, l1_ratio = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_dim = 599*4\n",
    "        self.out_dim = out_dim\n",
    "        self.next_dim = next_dim\n",
    "        self.encoder = ENEncoder(self.in_dim, self.out_dim, l1_ratio)\n",
    "        self.mlp = MLP(input_dim=out_dim, hidden_dim=hidden_dim, num_layers=num_layers, next_dim=next_dim)\n",
    "\n",
    "    def forward(self, x, onehot):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        x_encoded = self.encoder(x_flat)\n",
    "        return self.mlp(x_encoded, onehot)\n",
    "\n",
    "    def elasticnet_penalty(self):\n",
    "        return self.encoder.elasticnet_penalty()\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super(MAPELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs((y_true - y_pred) / (y_true + self.epsilon))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5bb49d-9c52-42ac-9025-6476c82a5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_EN:\n",
    "    def __init__(self, model, lr=1e-3, weight_decay=0, epoch=1000, patience=50, random_seed = 0, alpha = 1e-3):\n",
    "        self.random_seed = random_seed\n",
    "        #setRandomSeed(self.random_seed)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.cri2 = MAPELoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.8)\n",
    "        self.epoch = epoch\n",
    "        self.patience = patience\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def train(self, train_loader, test_loader=None, val_loader=None):\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for ep in range(1, self.epoch + 1):\n",
    "            self.model.train()\n",
    "            for x_batch, onehot, y_batch in train_loader:\n",
    "                x_batch, onehot, y_batch = x_batch.to(self.device), onehot.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x_batch, onehot)\n",
    "                base_loss = self.criterion(outputs, y_batch)\n",
    "                reg_loss = self.model.elasticnet_penalty()\n",
    "                loss = base_loss + self.alpha * reg_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            avg_train_loss = self.evaluate(train_loader)\n",
    "            test_loss = self.evaluate(test_loader) if test_loader else None\n",
    "            val_loss = self.evaluate(val_loader) if val_loader else None\n",
    "\n",
    "            self.train_loss.append(avg_train_loss)\n",
    "            self.test_loss.append(test_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "\n",
    "            if val_loader:\n",
    "                if val_loss < best_val_loss - 1e-4:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict()\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= self.patience:\n",
    "                        print(f\"Early stopping at epoch {ep}\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, onehot, y_batch in data_loader:\n",
    "                x_batch, onehot, y_batch = x_batch.to(self.device), onehot.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(x_batch, onehot)\n",
    "                loss = self.cri2(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def predict(self, x, onehot):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = x.to(self.device)\n",
    "            onehot = onehot.to(self.device)\n",
    "            return self.model(x, onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5ce045a-3d1d-46fe-87c8-37ac5f18f8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 62\n",
      "Model: EN, rs: 100, l1: 0.1\n",
      " Train MAPE: 2.11%, Val MAPE: 3.13%, Test MAPE: 2.16%\n",
      "Test MAPE by 'Next': M: 2.25%, D: 1.77%, H: 2.47%\n",
      "3.52 s\n",
      "Early stopping at epoch 55\n",
      "Model: EN, rs: 100, l1: 0.3\n",
      " Train MAPE: 1.53%, Val MAPE: 1.28%, Test MAPE: 1.43%\n",
      "Test MAPE by 'Next': M: 1.57%, D: 1.36%, H: 1.36%\n",
      "2.06 s\n",
      "Early stopping at epoch 79\n",
      "Model: EN, rs: 100, l1: 0.5\n",
      " Train MAPE: 1.32%, Val MAPE: 1.14%, Test MAPE: 1.20%\n",
      "Test MAPE by 'Next': M: 1.23%, D: 1.12%, H: 1.25%\n",
      "2.85 s\n",
      "Early stopping at epoch 91\n",
      "Model: EN, rs: 100, l1: 0.7\n",
      " Train MAPE: 1.38%, Val MAPE: 1.02%, Test MAPE: 1.24%\n",
      "Test MAPE by 'Next': M: 1.25%, D: 1.19%, H: 1.27%\n",
      "3.28 s\n",
      "Early stopping at epoch 97\n",
      "Model: EN, rs: 100, l1: 0.9\n",
      " Train MAPE: 1.49%, Val MAPE: 1.20%, Test MAPE: 1.37%\n",
      "Test MAPE by 'Next': M: 1.47%, D: 1.34%, H: 1.31%\n",
      "3.55 s\n",
      "Early stopping at epoch 150\n",
      "Model: EN, rs: 120, l1: 0.1\n",
      " Train MAPE: 1.90%, Val MAPE: 1.71%, Test MAPE: 1.80%\n",
      "Test MAPE by 'Next': M: 1.67%, D: 1.78%, H: 1.95%\n",
      "5.34 s\n",
      "Early stopping at epoch 65\n",
      "Model: EN, rs: 120, l1: 0.3\n",
      " Train MAPE: 1.80%, Val MAPE: 1.68%, Test MAPE: 1.49%\n",
      "Test MAPE by 'Next': M: 1.49%, D: 1.40%, H: 1.56%\n",
      "2.54 s\n",
      "Early stopping at epoch 75\n",
      "Model: EN, rs: 120, l1: 0.5\n",
      " Train MAPE: 1.39%, Val MAPE: 1.15%, Test MAPE: 1.15%\n",
      "Test MAPE by 'Next': M: 1.09%, D: 1.12%, H: 1.25%\n",
      "2.79 s\n",
      "Early stopping at epoch 71\n",
      "Model: EN, rs: 120, l1: 0.7\n",
      " Train MAPE: 1.86%, Val MAPE: 1.66%, Test MAPE: 1.60%\n",
      "Test MAPE by 'Next': M: 1.56%, D: 1.66%, H: 1.59%\n",
      "2.67 s\n",
      "Early stopping at epoch 71\n",
      "Model: EN, rs: 120, l1: 0.9\n",
      " Train MAPE: 2.90%, Val MAPE: 2.63%, Test MAPE: 2.31%\n",
      "Test MAPE by 'Next': M: 1.98%, D: 2.06%, H: 2.88%\n",
      "2.65 s\n",
      "Early stopping at epoch 68\n",
      "Model: EN, rs: 140, l1: 0.1\n",
      " Train MAPE: 1.97%, Val MAPE: 1.82%, Test MAPE: 1.96%\n",
      "Test MAPE by 'Next': M: 2.18%, D: 1.92%, H: 1.78%\n",
      "2.93 s\n",
      "Early stopping at epoch 63\n",
      "Model: EN, rs: 140, l1: 0.3\n",
      " Train MAPE: 2.61%, Val MAPE: 3.24%, Test MAPE: 3.00%\n",
      "Test MAPE by 'Next': M: 2.71%, D: 3.01%, H: 3.27%\n",
      "2.67 s\n",
      "Early stopping at epoch 72\n",
      "Model: EN, rs: 140, l1: 0.5\n",
      " Train MAPE: 1.24%, Val MAPE: 1.31%, Test MAPE: 1.25%\n",
      "Test MAPE by 'Next': M: 1.32%, D: 1.22%, H: 1.22%\n",
      "2.73 s\n",
      "Early stopping at epoch 74\n",
      "Model: EN, rs: 140, l1: 0.7\n",
      " Train MAPE: 1.90%, Val MAPE: 2.37%, Test MAPE: 2.14%\n",
      "Test MAPE by 'Next': M: 1.93%, D: 2.20%, H: 2.30%\n",
      "2.91 s\n",
      "Early stopping at epoch 73\n",
      "Model: EN, rs: 140, l1: 0.9\n",
      " Train MAPE: 1.32%, Val MAPE: 1.46%, Test MAPE: 1.32%\n",
      "Test MAPE by 'Next': M: 1.35%, D: 1.27%, H: 1.34%\n",
      "2.74 s\n",
      "Early stopping at epoch 62\n",
      "Model: EN, rs: 160, l1: 0.1\n",
      " Train MAPE: 1.68%, Val MAPE: 1.66%, Test MAPE: 1.66%\n",
      "Test MAPE by 'Next': M: 1.65%, D: 1.59%, H: 1.74%\n",
      "2.38 s\n",
      "Early stopping at epoch 68\n",
      "Model: EN, rs: 160, l1: 0.3\n",
      " Train MAPE: 2.11%, Val MAPE: 2.30%, Test MAPE: 2.06%\n",
      "Test MAPE by 'Next': M: 2.35%, D: 1.94%, H: 1.90%\n",
      "2.61 s\n",
      "Early stopping at epoch 92\n",
      "Model: EN, rs: 160, l1: 0.5\n",
      " Train MAPE: 2.97%, Val MAPE: 2.89%, Test MAPE: 2.89%\n",
      "Test MAPE by 'Next': M: 3.03%, D: 2.68%, H: 2.95%\n",
      "3.41 s\n",
      "Early stopping at epoch 89\n",
      "Model: EN, rs: 160, l1: 0.7\n",
      " Train MAPE: 3.50%, Val MAPE: 3.52%, Test MAPE: 3.47%\n",
      "Test MAPE by 'Next': M: 3.86%, D: 3.26%, H: 3.28%\n",
      "3.40 s\n",
      "Early stopping at epoch 150\n",
      "Model: EN, rs: 160, l1: 0.9\n",
      " Train MAPE: 1.27%, Val MAPE: 1.42%, Test MAPE: 1.21%\n",
      "Test MAPE by 'Next': M: 1.31%, D: 1.10%, H: 1.23%\n",
      "5.53 s\n",
      "Early stopping at epoch 95\n",
      "Model: EN, rs: 180, l1: 0.1\n",
      " Train MAPE: 1.04%, Val MAPE: 1.05%, Test MAPE: 1.11%\n",
      "Test MAPE by 'Next': M: 1.16%, D: 1.13%, H: 1.05%\n",
      "3.57 s\n",
      "Early stopping at epoch 112\n",
      "Model: EN, rs: 180, l1: 0.3\n",
      " Train MAPE: 2.18%, Val MAPE: 1.89%, Test MAPE: 2.38%\n",
      "Test MAPE by 'Next': M: 2.36%, D: 2.31%, H: 2.46%\n",
      "4.42 s\n",
      "Early stopping at epoch 113\n",
      "Model: EN, rs: 180, l1: 0.5\n",
      " Train MAPE: 1.30%, Val MAPE: 1.26%, Test MAPE: 1.27%\n",
      "Test MAPE by 'Next': M: 1.35%, D: 1.23%, H: 1.24%\n",
      "4.15 s\n",
      "Early stopping at epoch 108\n",
      "Model: EN, rs: 180, l1: 0.7\n",
      " Train MAPE: 1.37%, Val MAPE: 1.36%, Test MAPE: 1.63%\n",
      "Test MAPE by 'Next': M: 1.52%, D: 1.65%, H: 1.73%\n",
      "4.00 s\n",
      "Early stopping at epoch 95\n",
      "Model: EN, rs: 180, l1: 0.9\n",
      " Train MAPE: 2.20%, Val MAPE: 2.27%, Test MAPE: 2.04%\n",
      "Test MAPE by 'Next': M: 2.03%, D: 1.94%, H: 2.13%\n",
      "3.65 s\n"
     ]
    }
   ],
   "source": [
    "random_states = [100, 120, 140, 160, 180]\n",
    "bs = 12\n",
    "ep = 1000\n",
    "hid = 16\n",
    "layer = 5\n",
    "lr = 1e-3 \n",
    "\n",
    "l1s = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "results_df = pd.DataFrame(columns = ['rs', 'l1', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Next M MAPE', 'Next D MAPE', 'Next H MAPE'])\n",
    "\n",
    "X, y = Get_Data_all(dat)\n",
    "\n",
    "for rs, l1 in product(random_states, l1s):\n",
    "    setRandomSeed(rs)\n",
    "    \n",
    "    start = time.time()\n",
    "    X_tn, X_te, y_tn, y_te = Even_Split(X, y, 1/3, rs)\n",
    "    train_loader, val_loader, test_loader, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor, scaler = convert_to_tensor(X_tn, X_te, y_tn, y_te, bs, 1/6, rs)\n",
    "    \n",
    "    model = EN_MLP(4, hid, layer, 3, l1)\n",
    "    trainer = Trainer_EN(model, lr=lr, epoch = ep, random_seed = rs)\n",
    "    \n",
    "    trainer.train(train_loader, val_loader, test_loader)\n",
    "\n",
    "    info = [rs, l1]\n",
    "    results = plot_results(\"EN\", info, train_loader, val_loader, test_loader, plot = False)\n",
    "\n",
    "    temp_df = pd.DataFrame(info+list(results)).T\n",
    "    temp_df.columns = ['rs', 'l1', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Next M MAPE', 'Next D MAPE', 'Next H MAPE']\n",
    "    results_df = pd.concat([results_df, temp_df])\n",
    "    print(f\"{time.time()-start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6d18e5-0dd9-45a1-8bc8-5deabd574e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>Train MAPE</th>\n",
       "      <th>Val MAPE</th>\n",
       "      <th>Test MAPE</th>\n",
       "      <th>Next M MAPE</th>\n",
       "      <th>Next D MAPE</th>\n",
       "      <th>Next H MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.738808</td>\n",
       "      <td>1.873823</td>\n",
       "      <td>1.738977</td>\n",
       "      <td>1.780028</td>\n",
       "      <td>1.638202</td>\n",
       "      <td>1.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.046160</td>\n",
       "      <td>2.077415</td>\n",
       "      <td>2.070527</td>\n",
       "      <td>2.095910</td>\n",
       "      <td>2.003427</td>\n",
       "      <td>2.112243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.644468</td>\n",
       "      <td>1.552144</td>\n",
       "      <td>1.553407</td>\n",
       "      <td>1.603984</td>\n",
       "      <td>1.473725</td>\n",
       "      <td>1.582513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2.001730</td>\n",
       "      <td>1.988089</td>\n",
       "      <td>2.017828</td>\n",
       "      <td>2.025146</td>\n",
       "      <td>1.993603</td>\n",
       "      <td>2.034734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1.837555</td>\n",
       "      <td>1.796024</td>\n",
       "      <td>1.650253</td>\n",
       "      <td>1.626557</td>\n",
       "      <td>1.543016</td>\n",
       "      <td>1.781187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    l1  Train MAPE  Val MAPE  Test MAPE  Next M MAPE  Next D MAPE  Next H MAPE\n",
       "0  0.1    1.738808  1.873823   1.738977     1.780028     1.638202     1.798700\n",
       "1  0.3    2.046160  2.077415   2.070527     2.095910     2.003427     2.112243\n",
       "2  0.5    1.644468  1.552144   1.553407     1.603984     1.473725     1.582513\n",
       "3  0.7    2.001730  1.988089   2.017828     2.025146     1.993603     2.034734\n",
       "4  0.9    1.837555  1.796024   1.650253     1.626557     1.543016     1.781187"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = results_df.groupby(['l1'])[['Train MAPE', 'Val MAPE', 'Test MAPE', 'Next M MAPE', 'Next D MAPE', 'Next H MAPE']].mean().reset_index()\n",
    "summary_df\n",
    "summary_df.to_csv('EN_next_info_sum.csv')\n",
    "results_df.to_csv('EN_next_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
